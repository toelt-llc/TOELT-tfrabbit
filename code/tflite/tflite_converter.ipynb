{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow to Tensorflow Lite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I try to create a simple image recognition network in order to then covert it to TF Lite to compare results.\n",
    "The original network is tested and saved, with tf.saved_model.save. It is recommended to use this rather than the Keras API when trying to convert the model to TF Lite.\n",
    "So far, the model is very accurate in the classical version, it works and predicts with the Lite version too. \n",
    "\n",
    "\n",
    "Currently it uses a loop to predict the every img of the test set 1 by 1, there must be a more efficient way to do so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Tensorflow normal model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "model1 = Sequential([\n",
    "    Flatten(input_shape=(784,)),\n",
    "    Dense(320, activation='relu'),\n",
    "    Dense(160, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model1.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                optimizer = 'sgd',\n",
    "                metrics = 'acc')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data \n",
    "The data only uses one original 'train' data set from MNIST Kaggle because the test set was an exercise one and had no labels given so it is not usable for the accuracy results we want here"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = pd.read_csv('./mnist_data/train.csv')\n",
    "df_train = df.iloc[:35000] # 35000/42000\n",
    "df_test2 = df.iloc[35000:] # the 7000 left\n",
    "df_test= df_test2.drop('label', axis = 1)\n",
    "\n",
    "# Has the label column\n",
    "print('test shape : ', df_test.shape)\n",
    "\n",
    "\n",
    "X = df_train.drop('label', axis = 1)\n",
    "X /= 255.0 # Normalize\n",
    "#print(X.loc[0].shape)\n",
    "\n",
    "# Label of all the images\n",
    "y = df_train['label']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test shape :  (7000, 784)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# The nb of epochs doesn't matter much at the moment, at 30 accuracy is at 99% \n",
    "model1.fit(X, y, epochs=30)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.7425 - acc: 0.8145\n",
      "Epoch 2/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.3300 - acc: 0.9070\n",
      "Epoch 3/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.2733 - acc: 0.9221\n",
      "Epoch 4/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.2390 - acc: 0.9305\n",
      "Epoch 5/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.2125 - acc: 0.9388\n",
      "Epoch 6/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1915 - acc: 0.9449\n",
      "Epoch 7/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1735 - acc: 0.9504\n",
      "Epoch 8/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1585 - acc: 0.9555\n",
      "Epoch 9/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1453 - acc: 0.9592\n",
      "Epoch 10/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1336 - acc: 0.9624\n",
      "Epoch 11/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1241 - acc: 0.9653\n",
      "Epoch 12/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1151 - acc: 0.9683\n",
      "Epoch 13/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1068 - acc: 0.9711\n",
      "Epoch 14/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1006 - acc: 0.9723\n",
      "Epoch 15/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0934 - acc: 0.9748\n",
      "Epoch 16/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0878 - acc: 0.9766\n",
      "Epoch 17/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0824 - acc: 0.9775\n",
      "Epoch 18/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0773 - acc: 0.9790\n",
      "Epoch 19/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0727 - acc: 0.9806\n",
      "Epoch 20/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0687 - acc: 0.9814\n",
      "Epoch 21/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0646 - acc: 0.9829\n",
      "Epoch 22/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0608 - acc: 0.9837\n",
      "Epoch 23/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0575 - acc: 0.9848\n",
      "Epoch 24/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0545 - acc: 0.9860\n",
      "Epoch 25/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0514 - acc: 0.9867\n",
      "Epoch 26/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0485 - acc: 0.9877\n",
      "Epoch 27/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0459 - acc: 0.9885\n",
      "Epoch 28/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0431 - acc: 0.9890\n",
      "Epoch 29/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0409 - acc: 0.9905\n",
      "Epoch 30/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0385 - acc: 0.9913\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff869caedf0>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "ypreds = model1.predict(df_test)\n",
    "print('ypreds shape :', ypreds.shape)\n",
    "# ypreds is the prediction result over the test set, using the model1 (normal TF)\n",
    "# in one-hot format\n",
    "ypreds"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ypreds shape : (7000, 10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example plot/verification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "number = df_test.loc[35002].values\n",
    "number = number.reshape(28,28)\n",
    "plt.imshow(number)\n",
    "# a way of returning the only 1 in the 0s array : np.argsort(ypreds[2])[9]\n",
    "# a much better way\n",
    "np.argmax(ypreds[2])\n",
    "# the prediction is correct"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "It is saved here because it then easier to see how it re-usable for tflite use, and because the first conversion attempt is with tf.lite.TFLiteConverter.from_saved_model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "path = './model1_mnist/'\n",
    "tf.saved_model.save(model1, path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./model1_mnist/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. TFLite model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First we convert it "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# could use from_keras_model, may not be so much differences\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'serving_default': {'inputs': ['flatten_input'], 'outputs': ['dense_2']}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Then we set up the interpreter\n",
    "(See TFLite site)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "interpreter.allocate_tensors()\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Attempt to test with the same image as tested with the model network, to compare predictions.\n",
    "\n",
    "Careful on the input/output shape. As I didn't specify the model signatures, the converter understood the input shape as [1, 784]\n",
    "\n",
    "Note : the data type 'float32' also come from the signature, and could possibly be changed for optimization. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(input_details)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print('Signatures =>', signatures)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'serving_default_flatten_input:0', 'index': 0, 'shape': array([  1, 784], dtype=int32), 'shape_signature': array([ -1, 784], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Signatures => {'serving_default': {'inputs': ['flatten_input'], 'outputs': ['dense_2']}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "input_shape = input_details[0]['shape']\n",
    "print(input_shape)\n",
    "\n",
    "# Is set to 0 at the beginning\n",
    "print(interpreter.get_tensor(output_details[0]['index']))\n",
    "\n",
    "test_img = df_test.loc[35008].values # is a 9 \n",
    "input_data = np.array(test_img, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data.reshape(1,784))\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data) # predicts a 9 too \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  1 784]\n",
      "[[nan nan  0.  0.  0.  0. nan nan  0.  0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optional : "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Deprecated \n",
    "# Is a 'slow' way to invoke the interpreter on every test image and then create a new result list. \n",
    "# Was needed when I didn't know how to change the input shape of the input tensor.\n",
    "litepreds = []\n",
    "for j in range(df_test.shape[0]):\n",
    "    row = df_test.loc[35000+j].values\n",
    "    inputimg = np.array(row, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], inputimg.reshape(1,784))\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    litepreds.append(output_data)\n",
    "\n",
    "print(litepreds[5])\n",
    "print(ypreds[5])\n",
    "truth = np.array(litepreds == ypreds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "input_details[0]['index']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now I want to run the inference on the whole trainining set and not just one image at each interpreter 'invoke'.\n",
    "The first step is converting the df to an array of the right data type in order to input it to the tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "testarray = df_test.to_numpy()\n",
    "#np.shape(testarray)\n",
    "testarray = np.array(testarray, dtype=np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then I run the predictions on the whole set, efter resizing the input tensor, allocating and loading the new 'testarray' into the the input tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(7000,784))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], testarray)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "tflite_predictions = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Results obtained"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#check\n",
    "np.shape(tflite_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "a = np.argmax(tflite_predictions, axis=1)\n",
    "b = np.argmax(ypreds, axis=1)\n",
    "\n",
    "# correct labels are on the last column from the df_test2 set, df_test had it dropped for size issues\n",
    "labels = df_test2.iloc[:,0].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "assert np.shape(a) == np.shape(b) == (7000,) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "(b==labels).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9694285714285714"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classical vs lite accuracy, will be 1 if no changes to the data type, meaning we juste have the same model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "(a==b).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "interpreter.tensor(output_details[0]['index'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tensorflow.lite.python.interpreter.Interpreter.tensor.<locals>.<lambda>()>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Quantization\n",
    "Starting with Post-training integer quantization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# converter2 = tf.lite.TFLiteConverter.from_keras_model(model1)\n",
    "# converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# tflite_model_quant = converter2.convert()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format.\n",
    "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a RepresentativeDatase, a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.) To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Normalize the input image so that each pixel value is between 0 to 1. (was done earlier in the first model fit)\n",
    "df_train2 = df_train.astype(np.float32) / 255.0\n",
    "df_train2 = df_train2.drop('label', axis = 1, errors='ignore')\n",
    "df_train2 = df_train2.astype(np.float32)\n",
    "df_test2 = df_test2.astype(np.float32) / 255.0\n",
    "df_train2.dtypes\n",
    "df_train2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35000 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "34995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "34996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "34997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "34998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "34999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "       pixel9  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4         0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "34995     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "34996     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "34997     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "34998     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "34999     0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0           0.0       0.0       0.0       0.0       0.0  \n",
       "1           0.0       0.0       0.0       0.0       0.0  \n",
       "2           0.0       0.0       0.0       0.0       0.0  \n",
       "3           0.0       0.0       0.0       0.0       0.0  \n",
       "4           0.0       0.0       0.0       0.0       0.0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "34995       0.0       0.0       0.0       0.0       0.0  \n",
       "34996       0.0       0.0       0.0       0.0       0.0  \n",
       "34997       0.0       0.0       0.0       0.0       0.0  \n",
       "34998       0.0       0.0       0.0       0.0       0.0  \n",
       "34999       0.0       0.0       0.0       0.0       0.0  \n",
       "\n",
       "[35000 rows x 784 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "dato = tf.data.Dataset.from_tensor_slices(df_train2).batch(1).take(100)\n",
    "print(dato)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<TakeDataset shapes: (None, 784), types: tf.float32>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(df_train2).batch(1).take(1000):\n",
    "    yield [input_value]\n",
    "\n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model1)\n",
    "converter2.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter2.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 \n",
    "converter2.inference_input_type = tf.uint8\n",
    "converter2.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter2.convert()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pj/_64wh_0d44z5f2y6mk2zml0w0000gn/T/tmpwdupgfqf/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/pj/_64wh_0d44z5f2y6mk2zml0w0000gn/T/tmpwdupgfqf/assets\n",
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Interpretre2\n",
    "interpreter2 = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type_q = interpreter2.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type_q)\n",
    "output_type_q = interpreter2.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type_q)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "signatures_q = interpreter2.get_signature_list()\n",
    "print(signatures_q)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "interpreter2.allocate_tensors()\n",
    "input_details_q = interpreter2.get_input_details()[0]\n",
    "output_details_q = interpreter2.get_output_details()[0]\n",
    "\n",
    "input_details_q # we can see the dtype changed to uint8 and the quantization parameters and settings\n",
    "output_details_q # ??? why is the index 14 "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'Identity',\n",
       " 'index': 14,\n",
       " 'shape': array([ 1, 10], dtype=int32),\n",
       " 'shape_signature': array([-1, 10], dtype=int32),\n",
       " 'dtype': numpy.uint8,\n",
       " 'quantization': (0.00390625, 0),\n",
       " 'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "  'zero_points': array([0], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "testarray2 = df_test.to_numpy()\n",
    "#np.shape(testarray)\n",
    "testarray2 = np.array(testarray2, dtype=np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "#output_details_q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "np.shape(testarray2)\n",
    "#testarray2[0]\n",
    "#testarray[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "if input_details_q['dtype'] == np.uint8:\n",
    "    input_scale, input_zero_point = input_details_q[\"quantization\"]\n",
    "    test_img1 =  testarray2[5]\n",
    "    test_img1 = test_img1 / input_scale + input_zero_point\n",
    "testarray2 = testarray2 / input_scale\n",
    "testarray2 = testarray2 + input_zero_point"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "#interpreter2.resize_tensor_input(input_index = input_details_q[0]['index'],tensor_size=(7000,784))\n",
    "interpreter2.allocate_tensors()\n",
    "\n",
    "test_img1 = test_img1.astype(input_details_q[\"dtype\"])\n",
    "interpreter2.set_tensor(0, test_img1.reshape(1,784))\n",
    "\n",
    "interpreter2.invoke()\n",
    "\n",
    "tflite_predictions_quant = interpreter2.get_tensor(14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "#test_img1\n",
    "#testarray2[5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "testarray0 = testarray2[5]\n",
    "testarray0 = testarray0.reshape(28,28)\n",
    "test_img1 = test_img1.reshape(28,28)\n",
    "plt.imshow(testarray0)\n",
    "#plt.imshow(test_img1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff84a831250>"
      ]
     },
     "metadata": {},
     "execution_count": 86
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "plt.imshow(test_img1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff84a844e20>"
      ]
     },
     "metadata": {},
     "execution_count": 87
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "print(np.shape(tflite_predictions_quant))\n",
    "tflite_predictions_quant"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  6,  19, 124,  27,  13,  27,  37,   1,   2,   1]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "pred = np.argmax(tflite_predictions_quant)\n",
    "pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# Whole test\n",
    "testarray2 = np.array(testarray2, dtype=np.uint8)\n",
    "interpreter2.resize_tensor_input(input_details[0]['index'],tensor_size=(7000,784))\n",
    "interpreter2.allocate_tensors()\n",
    "\n",
    "interpreter2.set_tensor(input_details[0]['index'], testarray2)\n",
    "\n",
    "interpreter2.invoke()\n",
    "\n",
    "tflite_predictions_quant2 = interpreter2.get_tensor(output_details[0]['index'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "c = np.argmax(tflite_predictions_quant2, axis=1)\n",
    "assert np.shape(a) == np.shape(b) == (7000,) \n",
    "\n",
    "\n",
    "print('Quantized model accuracy :',(c==labels).mean() )\n",
    "print('Classic model accuracy:', (b==labels).mean() )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Quantized model accuracy : 0.23157142857142857\n",
      "Classic model accuracy: 0.9694285714285714\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "np.shape(tflite_predictions_quant2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}