{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantization on MNIST data set, 2d attempt "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working as expected "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "print(tf.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Using tf MNIST data set this time \n",
    "mnist = tf.keras.datasets.mnist\n",
    "fash = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_images.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model to be quantized"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")\n",
    "eval = model2.evaluate(test_images, test_labels)\n",
    "print(\"test loss, test acc:\", eval)\n",
    "tf.saved_model.save(model2, './model2_mnist/')\n",
    "model2.save('./model2_keras/')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-13 11:09:55.908224: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-09-13 11:09:55.908473: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-09-13 11:09:55.987473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.5118 - accuracy: 0.8598 - val_loss: 0.1057 - val_accuracy: 0.9703\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1003 - accuracy: 0.9712 - val_loss: 0.0743 - val_accuracy: 0.9768\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0724 - accuracy: 0.9789 - val_loss: 0.0595 - val_accuracy: 0.9819\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0600 - accuracy: 0.9828 - val_loss: 0.0596 - val_accuracy: 0.9807\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0511 - accuracy: 0.9850 - val_loss: 0.0598 - val_accuracy: 0.9794\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9794\n",
      "test loss, test acc: [0.05982794985175133, 0.9793999791145325]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-09-13 11:10:35.598872: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./model2_mnist/assets\n",
      "INFO:tensorflow:Assets written to: ./model2_keras/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Quantization blueprint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "path = './model1_mnist/'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Conversion set-up \n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "model = converter.convert()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No quantization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "# Converted model interpreter \n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('Input details : ', input_details)\n",
    "print('Output details :', output_details)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input details :  [{'name': 'serving_default_flatten_input:0', 'index': 0, 'shape': array([  1, 784], dtype=int32), 'shape_signature': array([ -1, 784], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details : [{'name': 'StatefulPartitionedCall:0', 'index': 12, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the input shape is set to 1,784 ( from the original TF model) we resize the input tensor before allocating it again.   \n",
    "Then we can load the whole test set to produce a production result that we can interpret thanks to argmax."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "test_images = test_images.reshape(10000,784)\n",
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(10000,784))\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], test_images)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_predictions = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we get 98% prediction, which would be the precision o the noraml model, but no quantization was actually done : the datatype is till float32"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "pred = np.argmax(tflite_predictions, axis=1)\n",
    "print('Accuracy : ', (pred == test_labels).mean() )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy :  0.9822\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Post-training integer quantization\n",
    "\n",
    "Now we will reproduce the steps, but with the addition of the quantization steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(1000):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8#,  \n",
    "    # tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    # tf.lite.OpsSet.SELECT_TF_OPS  \n",
    "]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quant_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quant_model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# To replace in the interpreter.set_tensor line for single inference : \n",
    "\n",
    "# test_images[10] = np.expand_dims(test_images[10], axis=0).astype(input_type)\n",
    "# test_images[10] = test_images[10] / input_scale + input_zero_point\n",
    "# dici = test_images[10].astype(input_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Interpreter\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "#whole set \n",
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(10000,784))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_images_q = test_images / input_scale + input_zero_point\n",
    "test_images_q = np.reshape(test_images_q.astype(input_type), (10000,784))\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_images_q)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "# Invoke\n",
    "interpreter.invoke()\n",
    "tflite_predictions2 = interpreter.get_tensor(output_details[0]['index'])\n",
    "#tflite_predictions2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "pred2 = np.argmax(tflite_predictions2, axis=1)\n",
    "print('Accuracy : ', (pred2 == test_labels).mean())\n",
    "print('Predictions similarity noquant vs. quant : ', (pred2 == pred).mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy :  0.9821\n",
      "Predictions similarity noquant vs. quant :  0.9993\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# #plt.imshow(np.reshape(test_images_int8[97], (28,28)))\n",
    "# plt.show()\n",
    "# plt.imshow(np.reshape(test_images[10], (28,28)))\n",
    "# type(test_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Automated/Factored quantization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "fash = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Very important step\n",
    "train_images_norm = train_images.astype(np.float32) / 255.0\n",
    "test_images_norm = test_images.astype(np.float32) / 255.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model \n",
    "Here I can load any presaved model or train a new one"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Careful on which dataset model2 (and 1) are trained\n",
    "saved_model1 = './model1_mnist/'\n",
    "saved_model2 = './model2_mnist/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(train_images_norm).batch(1).take(1000):\n",
    "        yield [input_value]\n",
    "\n",
    "def conv(model_path):\n",
    "    # TODO set option to read it from saved model or from existing model\n",
    "    converter1 = tf.lite.TFLiteConverter.from_saved_model(model_path)\n",
    "    converter1.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter1.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter1.representative_dataset = representative_data_gen\n",
    "    converter1.inference_input_type = tf.uint8\n",
    "    converter1.inference_output_type = tf.uint8\n",
    "\n",
    "    quant_model = converter1.convert()\n",
    "    return(quant_model)\n",
    "\n",
    "\n",
    "def interpret(model, test_set):\n",
    "    interpreter = tf.lite.Interpreter(model_content=model)\n",
    "\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_type = interpreter.get_input_details()[0]['dtype']\n",
    "    # Quantization parameters : \n",
    "    input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "    # whole set, currently only supports test_set as an array\n",
    "    interpreter.resize_tensor_input(input_index=input_details[0]['index'], tensor_size=np.shape(test_set))\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # 8bit quantization approximation\n",
    "    test_images_q = test_set / input_scale + input_zero_point\n",
    "    test_images_q = np.reshape(test_images_q.astype(input_type), np.shape(test_set)) # wordy line\n",
    "\n",
    "    # Loading into the tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], test_images_q)\n",
    "\n",
    "    return interpreter\n",
    "\n",
    "def run_inference(interpreter):\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.invoke()\n",
    "    inference = interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions = np.argmax(inference, axis=1)\n",
    "    print('Quantized model accuracy : ', (predictions == test_labels).mean())\n",
    "\n",
    "    return predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Execution "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "# possible TODO : make it only in a single function, with (saved_model, test_images) params\n",
    "# + the dataset as a param\n",
    "# + \n",
    "quanted_model = conv(saved_model2)\n",
    "interpreter = interpret(quanted_model, test_images_norm)\n",
    "run_inference(interpreter)\n",
    "\n",
    "print('Original model accuracy : {}'.format(round(eval[1],2)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Quantized model accuracy :  0.9795\n",
      "Original model accuracy : 0.98\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### With model2 :\n",
    "on mnist   : accuracy 0.98\n",
    "on fashion : accuracy 0.89"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3b86509d9e092a22f159f8076743bb9267110d2fd0258ee7c56b362bcf9349"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('pienv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}