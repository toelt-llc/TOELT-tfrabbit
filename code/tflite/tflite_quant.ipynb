{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Quantization on MNIST data set, 2d attempt "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working as expected "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "# Using tf MNIST data set this time \n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2nd Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model2.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model2.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")\n",
    "\n",
    "tf.saved_model.save(model2, './model2_mnist/')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2755 - accuracy: 0.9246 - val_loss: 0.1207 - val_accuracy: 0.9665\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1100 - accuracy: 0.9682 - val_loss: 0.0855 - val_accuracy: 0.9741\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0825 - accuracy: 0.9762 - val_loss: 0.0711 - val_accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0689 - accuracy: 0.9795 - val_loss: 0.0670 - val_accuracy: 0.9785\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0604 - accuracy: 0.9821 - val_loss: 0.0607 - val_accuracy: 0.9800\n",
      "INFO:tensorflow:Assets written to: ./model2_mnist/assets\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Assets written to: ./model2_mnist/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Quantization blueprint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "path = './model1_mnist/'\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "# Conversion set-up \n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "model = converter.convert()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No quantization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "# Converted model interpreter \n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print('Input details : ', input_details)\n",
    "print('Output details :', output_details)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input details :  [{'name': 'serving_default_flatten_input:0', 'index': 0, 'shape': array([  1, 784], dtype=int32), 'shape_signature': array([ -1, 784], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Output details : [{'name': 'StatefulPartitionedCall:0', 'index': 12, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([-1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the input shape is set to 1,784 ( from the original TF model) we resize the input tensor before allocating it again.   \n",
    "Then we can load the whole test set to produce a production result that we can interpret thanks to argmax."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "test_images = test_images.reshape(10000,784)\n",
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(10000,784))\n",
    "interpreter.allocate_tensors()\n",
    "interpreter.set_tensor(input_details[0]['index'], test_images)\n",
    "\n",
    "interpreter.invoke()\n",
    "tflite_predictions = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we get 98% prediction, which would be the precision o the noraml model, but no quantization was actually done : the datatype is till float32"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "pred = np.argmax(tflite_predictions, axis=1)\n",
    "print('Accuracy : ', (pred == test_labels).mean() )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy :  0.9822\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Post-training integer quantization\n",
    "\n",
    "Now we will reproduce the steps, but with the addition of the quantization steps"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(data).batch(1).take(1000):\n",
    "    yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  \n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  \n",
    "]\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "quant_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=quant_model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "# To replace in the interpreter.set_tensor line for single inference : \n",
    "\n",
    "# test_images[10] = np.expand_dims(test_images[10], axis=0).astype(input_type)\n",
    "# test_images[10] = test_images[10] / input_scale + input_zero_point\n",
    "# dici = test_images[10].astype(input_type)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "# Interpreter\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "\n",
    "#whole set \n",
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(10000,784))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_images_q = test_images / input_scale + input_zero_point\n",
    "test_images_q = np.reshape(test_images_q.astype(input_type), (10000,784))\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], test_images_q)\n",
    "\n",
    "# Invoke\n",
    "interpreter.invoke()\n",
    "tflite_predictions2 = interpreter.get_tensor(output_details[0]['index'])\n",
    "tflite_predictions2"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ..., 255,   0,   0],\n",
       "       [  0,   0, 255, ...,   0,   0,   0],\n",
       "       [  0, 255,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "pred2 = np.argmax(tflite_predictions2, axis=1)\n",
    "print('Accuracy : ', (pred2 == test_labels).mean())\n",
    "print('Predictions similarity noquant vs. quant : ', (pred2 == pred).mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy :  0.9821\n",
      "Predictions similarity noquant vs. quant :  0.9993\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# #plt.imshow(np.reshape(test_images_int8[97], (28,28)))\n",
    "# plt.show()\n",
    "# plt.imshow(np.reshape(test_images[10], (28,28)))\n",
    "# type(test_images)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}