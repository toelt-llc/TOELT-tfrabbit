{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tensorflow to Tensorflow Lite"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Currently not working due to bad variables name and cells order"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, I try to create a simple image recognition network in order to then covert it to TF Lite to compare results.\n",
    "The original network is tested and saved, with tf.saved_model.save. It is recommended to use this rather than the Keras API when trying to convert the model to TF Lite.\n",
    "So far, the model is very accurate in the classical version, it works and predicts with the Lite version too. \n",
    "\n",
    "\n",
    "Currently it uses a loop to predict the every img of the test set 1 by 1, there must be a more efficient way to do so."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Tensorflow normal model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model1 = Sequential([\n",
    "    Flatten(input_shape=(784,)),\n",
    "    Dense(320, activation='relu'),\n",
    "    Dense(160, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model1.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                optimizer = 'sgd',\n",
    "                metrics = 'acc')\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Data "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "df = pd.read_csv('./mnist_data/train.csv')\n",
    "df_train = df.iloc[:35000] # 35000/42000\n",
    "df_test2 = df.iloc[35000:] # the 7000 left\n",
    "df_test= df_test2.drop('label', axis = 1)\n",
    "\n",
    "# Has the label column\n",
    "print('test shape : ', df_test.shape)\n",
    "\n",
    "\n",
    "X = df_train.drop('label', axis = 1)\n",
    "X /= 255.0 # Normalize\n",
    "#print(X.loc[0].shape)\n",
    "\n",
    "# Label of all the images\n",
    "y = df_train['label']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test shape :  (7000, 784)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model fit"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# The nb of epochs doesn't matter much at the moment, at 30 accuracy is at 99% \n",
    "model1.fit(X, y, epochs=30)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.7324 - acc: 0.8227\n",
      "Epoch 2/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.3285 - acc: 0.9068\n",
      "Epoch 3/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.2722 - acc: 0.9219\n",
      "Epoch 4/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.2385 - acc: 0.9319\n",
      "Epoch 5/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.2121 - acc: 0.9403\n",
      "Epoch 6/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1908 - acc: 0.9459\n",
      "Epoch 7/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1732 - acc: 0.9503\n",
      "Epoch 8/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1578 - acc: 0.9555\n",
      "Epoch 9/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1450 - acc: 0.9594\n",
      "Epoch 10/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1334 - acc: 0.9619\n",
      "Epoch 11/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.1232 - acc: 0.9651\n",
      "Epoch 12/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1143 - acc: 0.9675\n",
      "Epoch 13/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.1063 - acc: 0.9703\n",
      "Epoch 14/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0996 - acc: 0.9729\n",
      "Epoch 15/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0930 - acc: 0.9745\n",
      "Epoch 16/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0872 - acc: 0.9763\n",
      "Epoch 17/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0819 - acc: 0.9779\n",
      "Epoch 18/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0767 - acc: 0.9796\n",
      "Epoch 19/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0720 - acc: 0.9810\n",
      "Epoch 20/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0678 - acc: 0.9821\n",
      "Epoch 21/30\n",
      "1094/1094 [==============================] - 2s 2ms/step - loss: 0.0645 - acc: 0.9823\n",
      "Epoch 22/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0606 - acc: 0.9845\n",
      "Epoch 23/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0572 - acc: 0.9856\n",
      "Epoch 24/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0540 - acc: 0.9865\n",
      "Epoch 25/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0511 - acc: 0.9875\n",
      "Epoch 26/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0483 - acc: 0.9885\n",
      "Epoch 27/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0458 - acc: 0.9893\n",
      "Epoch 28/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0435 - acc: 0.9896\n",
      "Epoch 29/30\n",
      "1094/1094 [==============================] - 1s 1ms/step - loss: 0.0414 - acc: 0.9903\n",
      "Epoch 30/30\n",
      "1094/1094 [==============================] - 2s 1ms/step - loss: 0.0390 - acc: 0.9915\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe1608d3e20>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "ypreds = model1.predict(df_test)\n",
    "print('ypreds shape :', ypreds.shape)\n",
    "# ypreds is the prediction result over the test set, using the model1 (normal TF)\n",
    "# in one-hot format\n",
    "ypreds"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ypreds shape : (7000, 10)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example plot/verification"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "number = df_test.loc[35002].values\n",
    "number = number.reshape(28,28)\n",
    "plt.imshow(number)\n",
    "# a way of returning the only 1 in the 0s array : np.argsort(ypreds[2])[9]\n",
    "# a much better way\n",
    "np.argmax(ypreds[2])\n",
    "# the prediction is correct"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "metadata": {},
     "execution_count": 10
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the model\n",
    "It is saved here because it then easier to see how it re-usable for tflite use, and because the first conversion attempt is with tf.lite.TFLiteConverter.from_saved_model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "path = './model1_mnist/'\n",
    "tf.saved_model.save(model1, path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: ./model1_mnist/assets\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. TFLite model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First we convert it "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# could use from_keras_model, may not be so much differences\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'serving_default': {'inputs': ['flatten_input'], 'outputs': ['dense_2']}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Then we set up the interpreter\n",
    "(See TFLite site)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Attempt to test with the same image as tested with the model network, to compare predictions.\n",
    "\n",
    "Careful on the input/output shape. As I didn't specify the model signatures, the converter understood the input shape as [1, 784]\n",
    "\n",
    "Note : the data type 'float32' also come from the signature, and could possibly be changed for optimization. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(input_details)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print('Signatures =>', signatures)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'name': 'serving_default_flatten_input:0', 'index': 0, 'shape': array([  1, 784], dtype=int32), 'shape_signature': array([ -1, 784], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "Signatures => {'serving_default': {'inputs': ['flatten_input'], 'outputs': ['dense_2']}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "input_shape = input_details[0]['shape']\n",
    "print(input_shape)\n",
    "\n",
    "# Is set to 0 at the beginning\n",
    "print(interpreter.get_tensor(output_details[0]['index']))\n",
    "\n",
    "test_img = df_test.loc[35008].values # is a 9 \n",
    "input_data = np.array(test_img, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data.reshape(1,784))\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data) # predicts a 9 too \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  1 784]\n",
      "[[-0.2798224   0.28949627 -0.06875481 -0.1014585  -0.03039126  0.26040196\n",
      "   0.08608792  0.16431718  0.42715    -0.13336818]]\n",
      "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4249514e-22\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optional : "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Deprecated \n",
    "# Is a 'slow' way to invoke the interpreter on every test image and then create a new result list. \n",
    "# Was needed when I didn't know how to change the input shape of the input tensor.\n",
    "litepreds = []\n",
    "for j in range(df_test.shape[0]):\n",
    "    row = df_test.loc[35000+j].values\n",
    "    inputimg = np.array(row, dtype=np.float32)\n",
    "    interpreter.set_tensor(input_details[0]['index'], inputimg.reshape(1,784))\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    litepreds.append(output_data)\n",
    "\n",
    "print(litepreds[5])\n",
    "print(ypreds[5])\n",
    "truth = np.array(litepreds == ypreds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "input_details[0]['index']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Now I want to run the inference on the whole trainining set and not just one image at each interpreter 'invoke'.\n",
    "The first step is converting the df to an array of the right data type in order to input it to the tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "testarray = df_test.to_numpy()\n",
    "#np.shape(testarray)\n",
    "testarray = np.array(testarray, dtype=np.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then I run the predictions on the whole set, efter resizing the input tensor, allocating and loading the new 'testarray' into the the input tensor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "interpreter.resize_tensor_input(input_index = input_details[0]['index'],tensor_size=(7000,784))\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "interpreter.set_tensor(input_details[0]['index'], testarray)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "tflite_predictions = interpreter.get_tensor(output_details[0]['index'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Results obtained"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#check\n",
    "np.shape(tflite_predictions)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "a = np.argmax(tflite_predictions, axis=1)\n",
    "b = np.argmax(ypreds, axis=1)\n",
    "\n",
    "# correct labels are on the last column from the df_test2 set, df_test had it dropped for size issues\n",
    "labels = df_test2.iloc[:,0].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "assert np.shape(a) == np.shape(b) == (7000,) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "(b==labels).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9697142857142858"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classical vs lite accuracy, will be 1 if no changes to the data type, meaning we juste have the same model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "(a==b).mean()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "interpreter.tensor(output_details[0]['index'])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function tensorflow.lite.python.interpreter.Interpreter.tensor.<locals>.<lambda>()>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Quantization\n",
    "Starting with Post-training integer quantization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# converter2 = tf.lite.TFLiteConverter.from_keras_model(model1)\n",
    "# converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# tflite_model_quant = converter2.convert()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model is now a bit smaller with quantized weights, but other variable data is still in float format.\n",
    "To quantize the variable data (such as model input/output and intermediates between layers), you need to provide a RepresentativeDatase, a generator function that provides a set of input data that's large enough to represent typical values. It allows the converter to estimate a dynamic range for all the variable data. (The dataset does not need to be unique compared to the training or evaluation dataset.) To support multiple inputs, each representative data point is a list and elements in the list are fed to the model according to their indices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Normalize the input image so that each pixel value is between 0 to 1. (was done earlier in the first model fit)\n",
    "df_train2 = df_train.astype(np.float32) / 255.0\n",
    "df_train2 = df_train2.drop('label', axis = 1, errors='ignore')\n",
    "df_train2 = df_train2.astype(np.float32)\n",
    "df_test2 = df_test2.astype(np.float32) / 255.0\n",
    "df_train2.dtypes"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pixel0      float32\n",
       "pixel1      float32\n",
       "pixel2      float32\n",
       "pixel3      float32\n",
       "pixel4      float32\n",
       "             ...   \n",
       "pixel779    float32\n",
       "pixel780    float32\n",
       "pixel781    float32\n",
       "pixel782    float32\n",
       "pixel783    float32\n",
       "Length: 784, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(df_train2).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter2 = tf.lite.TFLiteConverter.from_saved_model(path)\n",
    "converter2.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.representative_dataset = representative_data_gen\n",
    "\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter2.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 \n",
    "converter2.inference_input_type = tf.uint8\n",
    "converter2.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant = converter2.convert()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:For model inputs containing unsupported operations which cannot be quantized, the `inference_input_type` attribute will default to the original type.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "## Quant results : \n",
    "interpreter2 = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type_q = interpreter2.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type_q)\n",
    "output_type_q = interpreter2.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type_q)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "\n",
    "signatures_q = interpreter2.get_signature_list()\n",
    "print(signatures_q)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'serving_default': {'inputs': ['flatten_input'], 'outputs': ['dense_2']}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "interpreter2.allocate_tensors()\n",
    "input_details_q = interpreter2.get_input_details()[0]\n",
    "output_details_q = interpreter2.get_output_details()[0]\n",
    "\n",
    "input_details_q # we can see the dtype changed to uint8 and the quantization parameters and settings"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'serving_default_flatten_input:0',\n",
       " 'index': 0,\n",
       " 'shape': array([  1, 784], dtype=int32),\n",
       " 'shape_signature': array([ -1, 784], dtype=int32),\n",
       " 'dtype': numpy.uint8,\n",
       " 'quantization': (0.003921568859368563, 0),\n",
       " 'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "  'zero_points': array([0], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "testarray2 = df_test.to_numpy()\n",
    "#np.shape(testarray)\n",
    "testarray2 = np.array(testarray2, dtype=np.uint8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "output_details_q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'StatefulPartitionedCall:0',\n",
       " 'index': 14,\n",
       " 'shape': array([ 1, 10], dtype=int32),\n",
       " 'shape_signature': array([-1, 10], dtype=int32),\n",
       " 'dtype': numpy.uint8,\n",
       " 'quantization': (0.00390625, 0),\n",
       " 'quantization_parameters': {'scales': array([0.00390625], dtype=float32),\n",
       "  'zero_points': array([0], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "np.shape(testarray2)\n",
    "#testarray2[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 784)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "if input_details_q['dtype'] == np.uint8:\n",
    "    input_scale, input_zero_point = input_details_q[\"quantization\"]\n",
    "    test_img1 =  testarray2[5]\n",
    "    test_img1 = test_img1 / input_scale + input_zero_point\n",
    "testarray2 = testarray2 / input_scale\n",
    "testarray2 = testarray2 + input_zero_point"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "#interpreter2.resize_tensor_input(input_index = input_details_q[0]['index'],tensor_size=(7000,784))\n",
    "interpreter2.allocate_tensors()\n",
    "\n",
    "test_img1 = test_img1.astype(input_details_q[\"dtype\"])\n",
    "interpreter2.set_tensor(0, test_img1.reshape(1,784))\n",
    "\n",
    "interpreter2.invoke()\n",
    "\n",
    "tflite_predictions_quant = interpreter2.get_tensor(14)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "input_details_q"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'serving_default_flatten_input:0',\n",
       " 'index': 0,\n",
       " 'shape': array([  1, 784], dtype=int32),\n",
       " 'shape_signature': array([ -1, 784], dtype=int32),\n",
       " 'dtype': numpy.uint8,\n",
       " 'quantization': (0.003921568859368563, 0),\n",
       " 'quantization_parameters': {'scales': array([0.00392157], dtype=float32),\n",
       "  'zero_points': array([0], dtype=int32),\n",
       "  'quantized_dimension': 0},\n",
       " 'sparsity_parameters': {}}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "test_img1\n",
    "testarray2[5]\n",
    "\n",
    "testarray0 = testarray2[5]\n",
    "testarray0 = testarray0.reshape(28,28)\n",
    "test_img1 = test_img1.reshape(28,28)\n",
    "plt.imshow(testarray0)\n",
    "#plt.imshow(test_img1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe140b0a910>"
      ]
     },
     "metadata": {},
     "execution_count": 38
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "np.shape(tflite_predictions_quant)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "pred = np.argmax(tflite_predictions_quant)\n",
    "pred"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# Whole test\n",
    "testarray2 = np.array(testarray2, dtype=np.uint8)\n",
    "interpreter2.resize_tensor_input(0,tensor_size=(7000,784))\n",
    "interpreter2.allocate_tensors()\n",
    "\n",
    "interpreter2.set_tensor(0, testarray2)\n",
    "\n",
    "interpreter2.invoke()\n",
    "\n",
    "tflite_predictions_quant2 = interpreter2.get_tensor(14)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "c = np.argmax(tflite_predictions_quant2, axis=1)\n",
    "assert np.shape(a) == np.shape(b) == (7000,) \n",
    "\n",
    "\n",
    "print('Quantized model accuracy :',(c==labels).mean() )\n",
    "print('Classic model accuracy:', (b==labels).mean() )"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Quantized model accuracy : 0.5912857142857143\n",
      "Classic model accuracy: 0.9697142857142858\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "np.shape(tflite_predictions_quant2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7000, 10)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}